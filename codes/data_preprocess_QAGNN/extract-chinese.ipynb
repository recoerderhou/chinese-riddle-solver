{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"relation_groups = [\n    'atlocation/locatednear',\n    'capableof',\n    'causes/causesdesire/*motivatedbygoal',\n    'createdby',\n    'desires',\n    'antonym/distinctfrom',\n    'hascontext',\n    'hasproperty',\n    'hassubevent/hasfirstsubevent/haslastsubevent/hasprerequisite/entails/mannerof',\n    'isa/instanceof/definedas',\n    'madeof',\n    'notcapableof',\n    'notdesires',\n    'partof/*hasa',\n    'relatedto/similarto/synonym',\n    'usedfor',\n    'receivesaction',\n]","metadata":{"execution":{"iopub.status.busy":"2021-12-17T15:03:20.749577Z","iopub.execute_input":"2021-12-17T15:03:20.750226Z","iopub.status.idle":"2021-12-17T15:03:20.756741Z","shell.execute_reply.started":"2021-12-17T15:03:20.75017Z","shell.execute_reply":"2021-12-17T15:03:20.755389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install zhconv","metadata":{"execution":{"iopub.status.busy":"2021-12-17T15:03:20.758359Z","iopub.execute_input":"2021-12-17T15:03:20.75895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import zhconv\nimport json","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_merge_relation():\n    relation_mapping = dict()\n    for line in relation_groups:\n        ls = line.strip().split('/')\n        rel = ls[0]\n        for l in ls:\n            if l.startswith(\"*\"):\n                relation_mapping[l[1:]] = \"*\" + rel\n            else:\n                relation_mapping[l] = rel\n    return relation_mapping","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def del_pos(s):\n    \"\"\"\n    Deletes part-of-speech encoding from an entity string, if present.\n    :param s: Entity string.\n    :return: Entity string with part-of-speech encoding removed.\n    \"\"\"\n    if s.endswith(\"/n\") or s.endswith(\"/a\") or s.endswith(\"/v\") or s.endswith(\"/r\"):\n        s = s[:-2]\n    return s\n\nprint(load_merge_relation())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_chinese(concept_path, output_csv_path, output_vocab_path):\n    print('extracting Chinese concepts and relations from ConceptNet...')\n    relation_mapping = load_merge_relation()\n    num_lines = sum(1 for line in open(conceptnet_path, 'r', encoding='utf-8'))\n    cpnet_vocab = []\n    concept_relation = []\n    concepts_seen = set()\n    with open(conceptnet_path, 'r', encoding=\"utf8\") as fin:\n        for line in fin:\n            toks = line.strip().split('\\t')\n            \n            if toks[2].startswith('/c/zh/') and toks[3].startswith('/c/zh/'):\n                \"\"\"\n                Some preprocessing:\n                    - Remove part-of-speech encoding.\n                    - Split(\"/\")[-1] to trim the \"/c/zh/\" and just get the entity name, convert all to \n                    - Lowercase for uniformity.\n                \"\"\"\n                rel = toks[1].split(\"/\")[-1].lower()\n                head = zhconv.convert(del_pos(toks[2]).split(\"/\")[-1],'zh-hans').lower()\n                tail = zhconv.convert(del_pos(toks[3]).split(\"/\")[-1],'zh-hans').lower()\n                \n                if rel not in relation_mapping:\n                    continue\n\n                rel = relation_mapping[rel]\n                if rel.startswith(\"*\"):\n                    head, tail, rel = tail, head, rel[1:]\n\n                data = json.loads(toks[4])\n\n                relation = '\\t'.join([rel, head, tail, str(data[\"weight\"])])\n                \n                concept_relation.append(relation)\n                for w in [head, tail]:\n                    if w not in concepts_seen:\n                        concepts_seen.add(w)\n                        cpnet_vocab.append(w)\n                        \n    with open(output_vocab_path, 'w+') as f:\n        for word in cpnet_vocab:\n            f.write(word + '\\n')\n    with open(output_csv_path, 'w+', encoding=\"utf8\") as fout:\n        for rela in concept_relation:\n            fout.write(rela + '\\n')\n            \n        \n\n    print(f'extracted ConceptNet csv file saved to {output_csv_path}')\n    print(f'extracted concept vocabulary saved to {output_vocab_path}')\n    print()\n    return cpnet_vocab, concept_relation","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conceptnet_path = '../input/raw-chineseconceptnet/chineseconceptnet.csv'\noutput_csv_path = 'conceptnet.zh.csv'\noutput_vocab_path = 'concept.txt'\noutput_path = 'matcher_patterns.zh.json'\n\nimport pandas as pd\n\nthings_to_write = ['why', 'this']\n\nwith open('somepath.txt', 'w+') as f:\n    for th in things_to_write:\n        f.write(th + '\\n')\n\ncpnet_vocab, concept_relation = extract_chinese(conceptnet_path, output_csv_path, output_vocab_path)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"pip install zhconv","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import zhconv\nprint(zhconv.convert('男仔', 'zh-hans'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nimport tqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_pattern(nlp, doc, debug=False):\n    pronoun_list = set([\"我\", \"你\", \"它\", \"它的\", \"你的\", \"他\", \"她\", \"他的\", \"她的\", \"他们\", \"他们的\", \"我们的\", \"我们\"])\n    # Filtering concepts consisting of all stop words and longer than four words.\n    if len(doc) >= 5 or doc[0].text in pronoun_list or doc[-1].text in pronoun_list:\n        if debug:\n            return False, doc.text\n        return None  # ignore this concept as pattern\n\n    pattern = []\n    for token in doc:  # a doc is a concept\n        pattern.append({\"LEMMA\": token.lemma_})\n    if debug:\n        return True, doc.text\n    return pattern","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_cpnet_vocab(cpnet_vocab_path):\n    with open(cpnet_vocab_path, \"r\", encoding=\"utf8\") as fin:\n        cpnet_vocab = [l.strip() for l in fin]\n    cpnet_vocab = [c.replace(\"_\", \" \") for c in cpnet_vocab]\n    return cpnet_vocab","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install spacy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip list","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install '../input/raw-chineseconceptnet/zh_core_web_sm-3.1.0/dist/zh_core_web_sm-3.1.0.tar'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_matcher_patterns(cpnet_vocab_path, output_path, debug=False):\n    cpnet_vocab = load_cpnet_vocab(cpnet_vocab_path)\n    nlp = spacy.load('zh_core_web_sm', disable=['parser', 'ner', 'textcat'])\n    docs = nlp.pipe(cpnet_vocab)\n    return\n    all_patterns = {}\n\n    if debug:\n        f = open(\"filtered_concept.txt\", \"w\")\n\n    for doc in docs:\n        print(doc.text)\n        pattern = create_pattern(nlp, doc, debug)\n        if debug:\n            if not pattern[0]:\n                f.write(pattern[1] + '\\n')\n\n        if pattern is None:\n            continue\n        all_patterns[\"_\".join(doc.text.split(\" \"))] = pattern\n\n    print(\"Created \" + str(len(all_patterns)) + \" patterns.\")\n    with open(output_path, \"w\", encoding=\"utf8\") as fout:\n        json.dump(all_patterns, fout)\n    if debug:\n        f.close()\n\ncreate_matcher_patterns(output_vocab_path, output_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}